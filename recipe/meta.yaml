{% set name = "onnx" %}
{% set version = "1.5.0" %}
{% set sha256 = "707887fffd79f36213baea7012fd1599c2c6d14ca4974484f1363dced4f7357a" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  fn: {{ name }}-{{ version }}.tar.gz
  url: https://github.com/onnx/onnx/archive/v{{ version }}.tar.gz
  sha256: {{ sha256 }}

build:
  number: 3
  script:
    - set "CL=/wd4244 /wd4251 /DPROTOBUF_USE_DLLS"  # [win]
    - set "CMAKE_GENERATOR_PLATFORM="  # [win]
    - set "CMAKE_GENERATOR=Visual Studio 14 2015"  # [win]
    - set "CMAKE_PREFIX_PATH=%LIBRARY_PREFIX%"   # [win]
    - set "ONNX_ML=1"  # [win]
    - export ONNX_ML=1  # [unix]
    # build script looks at this, but not set on
    - export CONDA_PREFIX="$PREFIX"  # [unix]
    - python -m pip install --no-deps --ignore-installed --verbose .
  entry_points:
    - check-model = onnx.bin.checker:check_model
    - check-node = onnx.bin.checker:check_node
    - backend-test-tools = onnx.backend.test.cmd_tools:main
  skip: true  # [win and vc<14]

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - cmake
  host:
    - python
    - pip
    - protobuf
    - pytest-runner
    - ninja
    - pybind11
  run:
    - python
    - protobuf
    - numpy
    - six
    - typing  # [py2k]

test:
  imports:
    - onnx
  commands:
    - check-model --help
    - check-node --help
    - backend-test-tools --help
    - conda inspect linkages -p $PREFIX {{ name }}  # [unix]
    - conda inspect objects -p $PREFIX {{ name }}   # [osx]

about:
  home: https://github.com/onnx/onnx/
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: Open Neural Network Exchange library
  description: |
    Open Neural Network Exchange (ONNX) is the first step toward an open
    ecosystem that empowers AI developers to choose the right tools as their
    project evolves. ONNX provides an open source format for AI models. It
    defines an extensible computation graph model, as well as definitions of
    built-in operators and standard data types. Initially we focus on the
    capabilities needed for inferencing (evaluation).

extra:
  recipe-maintainers:
    - ezyang
    - dougalsutherland
    - marcelotrevisani
